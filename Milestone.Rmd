---
title: "Capstone Milestone Report-Data Science"
author: "Yogitha"
date: "12/19/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview

This is the milestone report for the capstone project of the Coursera Data Scienence Specialization. The overall goal of the capstone is to develop a prediction algorithem for the most likely next word in a sequence of words. The purpose of this report is to demonstrate how data was downloaded, imported into R and cleaned. Furthermore it contains some exploratory analyses to investigate some features of the data.
We have been given the following text files:
-en_US.twitter.txt
-en_US.blogs.txt
-en_US.news.txt

## Data Pre-Processing

```{r }
setwd("C:/Users/yogitha.j.dodda/Desktop/Capstone-Project")
if(!file.exists("Coursera-SwiftKey.zip")){
  download.file("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip", "Coursera-SwiftKey.zip")
  unzip("Coursera-SwiftKey.zip")
}
blogs <- readLines("Coursera-SwiftKey/final/en_US/en_US.blogs.txt", warn = FALSE, encoding = "UTF-8")
news <- readLines("Coursera-SwiftKey/final/en_US/en_US.news.txt", warn = FALSE, encoding = "UTF-8")
twitter <- readLines("Coursera-SwiftKey/final/en_US/en_US.twitter.txt", warn = FALSE, encoding = "UTF-8")
```

## Summary Statistics

Calculate some summary statistics for each file: Size in Megabytes, Number of Entries (rows), Total Characters and Length of Longest Entry.

```{r}
summary <- data.frame('File' = c("Blogs","News","Twitter"),
                      "File Size" = sapply(list(blogs, news, twitter), function(x){format(object.size(x),"MB")}),
                      'Nentries' = sapply(list(blogs, news, twitter), function(x){length(x)}),
                      'TotalCharacters' = sapply(list(blogs, news, twitter), function(x){sum(nchar(x))}),
                      'MaxCharacters' = sapply(list(blogs, news, twitter), function(x){max(unlist(lapply(x, function(y) nchar(y))))})
                      )
summary
```

## Data Sampling and Cleaning

As the data is huge we will sample the data and use a subset. Then we are going to clean the data and convert to a corpus.

```{r}
set.seed(1313) 
sample_size <- 0.05 

blogs_index <- sample(seq_len(length(blogs)),length(blogs)*sample_size)
news_index <- sample(seq_len(length(news)),length(news)*sample_size)
twitter_index <- sample(seq_len(length(twitter)),length(twitter)*sample_size)

blogs_sub <- blogs[blogs_index[]]
news_sub <- news[news_index[]]
twitter_sub <- twitter[twitter_index[]]

library(tm) 

corpus <- Corpus(VectorSource(c(blogs_sub, news_sub, twitter_sub)), readerControl=list(reader=readPlain,language="en")) 

# Remove non ASCII characters
corpus <- Corpus(VectorSource(sapply(corpus, function(row) iconv(row, "latin1", "ASCII", sub="")))) 

corpus <- tm_map(corpus, removePunctuation) 
corpus <- tm_map(corpus, stripWhitespace) 
corpus <- tm_map(corpus, content_transformer(tolower)) 
corpus <- tm_map(corpus, removeNumbers) 
corpus <- tm_map(corpus, PlainTextDocument) 

```

## N-Grams Creation

Now that we have a clean dataset we need to convert it to a suitable format for Natural Language Prpcessing (NLP). The format of choice are N-grams stored in Term Document Matrices (TDM). The aim is to create a predictive model that can handle unigrams, bigrams, and trigrams. 

```{r}
library(RWeka) 

UnigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
QuadgramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 4, max = 4))

Unigrams <- TermDocumentMatrix(corpus, control = list(tokenize = UnigramTokenizer))
Bigrams <- TermDocumentMatrix(corpus, control = list(tokenize = BigramTokenizer))
Trigrams <- TermDocumentMatrix(corpus, control = list(tokenize = TrigramTokenizer))
Quadgrams <- TermDocumentMatrix(corpus, control = list(tokenize = QuadgramTokenizer))

Unigrams
Bigrams
Trigrams
Quadgrams
```

## Exploratory Data Analysis

The above matrices are extremely sparse (i.e. they are almost entirely composed of zeroes). We need to create denser matrices to do exploratory analyses and remove rare N-grams.

```{r}
library(ggplot2)

freq_frame <- function(tdm){
    freq <- sort(rowSums(as.matrix(tdm)), decreasing=TRUE)
    freq_frame <- data.frame(word=names(freq), freq=freq)
    return(freq_frame)
}

UnigramsDense <- removeSparseTerms(Unigrams, 0.999)
UnigramsDenseSorted <- freq_frame(UnigramsDense)
BigramsDense <- removeSparseTerms(Bigrams, 0.999)
BigramsDenseSorted <- freq_frame(BigramsDense)
TrigramsDense <- removeSparseTerms(Trigrams, 0.999)
TrigramsDenseSorted <- freq_frame(TrigramsDense)
QuadgramsDense <- removeSparseTerms(Quadgrams, 0.9999)
QuadgramsDenseSorted <- freq_frame(QuadgramsDense)

GG <- ggplot(data = UnigramsDenseSorted[1:40,], aes(x = reorder(word, -freq), y = freq)) + geom_bar(stat="identity",color='turquoise')
GG <- GG + labs(x = "N-gram", y = "Frequency", title = "Most Abundant Unigrams")
GG <- GG + theme(axis.text.x=element_text(angle=90))
GG
```

```{r}
GG <- ggplot(data = BigramsDenseSorted[1:40,], aes(x = reorder(word, -freq), y = freq)) + geom_bar(stat="identity",color='tomato')
GG <- GG + labs(x = "N-gram", y = "Frequency", title = "Most Abundant Bigrams")
GG <- GG + theme(axis.text.x=element_text(angle=90))
GG
```

```{r}
GG <- ggplot(data = TrigramsDenseSorted[1:40,], aes(x = reorder(word, -freq), y = freq)) + geom_bar(stat="identity",color='orchid')
GG <- GG + labs(x = "N-gram", y = "Frequency", title = "Most Abundant Trigrams")
GG <- GG + theme(axis.text.x=element_text(angle=90))
GG
```

```{r}
GG <- ggplot(data = QuadgramsDenseSorted[1:40,], aes(x = reorder(word, -freq), y = freq)) + geom_bar(stat="identity",color='yellowgreen')
GG <- GG + labs(x = "N-gram", y = "Frequency", title = "Most Abundant Quadgrams")
GG <- GG + theme(axis.text.x=element_text(angle=90))
GG
```

## Next Steps for Shiny App

Based on analysis, the next step is to use N-gram dataframe to calculate the probabilities of the next word depending on preceding input. For the Shiny app, the plan is to create an app with a simple interface where the user can enter a string of text. The prediction model will then show a list of most-probable next words.
